{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"XAI_Chatbot.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bbrsXYHvCDHo","colab_type":"text"},"source":["# XAI Chatbot\n","Python Client for Google Dialogflow API V2 <br>\n","Copyright 2020 Denis Rothman MIT License. See LICENSE."]},{"cell_type":"code","metadata":{"id":"hXhUmRRf946g","colab_type":"code","colab":{}},"source":["!pip install dialogflow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxbs7Yvz-Vmh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9fef5b0e-f2cb-437c-b386-5ff30de9a195"},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lj2PsZ6n-IZ5","colab_type":"code","colab":{}},"source":["import os\n","import dialogflow_v2 as dialogflow\n","from google.api_core.exceptions import InvalidArgument\n","\n","os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] ='['/content/'+YOUR PRIVATE KEY]'#'private_key.json'\n","DIALOGFLOW_PROJECT_ID = '[YOUR PROJECT ID]' #'[PROJECT_ID]' #Project ID\n","DIALOGFLOW_LANGUAGE_CODE ='en'       #'[LANGUAGE]'\n","SESSION_ID = '[YOUR PROJECT ID]'\n","\n","def dialog(our_query):\n","    #session variables\n","    session_client = dialogflow.SessionsClient()\n","    session = session_client.session_path(DIALOGFLOW_PROJECT_ID, SESSION_ID)\n","\n","    # Our query \n","    our_input = dialogflow.types.TextInput(text=our_query, language_code=DIALOGFLOW_LANGUAGE_CODE)\n","    query = dialogflow.types.QueryInput(text=our_input)\n","\n","    # try or raise exceptions\n","    try:\n","        response = session_client.detect_intent(session=session, query_input=query)\n","    except InvalidArgument:\n","        raise\n","\n","    return response.query_result.fulfillment_text\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mpqMtkG3aGGu","colab_type":"text"},"source":["# The Bellman equation, Q-learning, based on the Markov decision process(MDP) "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QYQ6UxAuhhVh"},"source":["## The reward matrix"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_uTqqXAChK-z","colab":{}},"source":["# Markov decision process (MDP) - The Bellman equations adapted to\n","# Q-learning. Reinforcement learning with the Q action-value(reward) function.\n","# Copyright 2019 Denis Rothman MIT License. See LICENSE.\n","import numpy as ql\n","# R is The Reward Matrix for each state\n","R = ql.matrix([ [0,0,0,0,1,0],\n","\t\t            [0,0,0,1,0,1],\n","\t\t            [0,0,100,1,0,0],\n","\t             \t[0,1,1,0,1,0],\n","\t\t            [1,0,0,1,0,0],\n","\t\t            [0,1,0,0,0,0] ])\n","\n","# Q is the Learning Matrix in which rewards will be learned/stored\n","Q = ql.matrix(ql.zeros([6,6]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UvdGSy26hr8p"},"source":["## The learning rate or training penalty"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DjM41MSZh4gB","colab":{}},"source":["# Gamma: It's a form of penalty or uncertainty for learning\n","# If the value is 1, the rewards would be too high.\n","# This way the system knows it is learning.\n","gamma = 0.8"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JE_o-xsJiL4g"},"source":["## Initial state"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KefpjG8DiKdR","colab":{}},"source":["# agent_s_state. The agent the name of the system calculating\n","# s is the state the agent is going from and s' the state it's going to\n","# this state can be random or it can be chosen as long as the rest of the choices\n","# are not determined. Randomness is part of this stochastic process\n","agent_s_state = 5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"stMsPTIfiUKZ"},"source":["## The random choice of the next state"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FzXY43JZieZA","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"dfac2f66-c8c9-4047-e88c-9a79369e2363"},"source":["# The possible \"a\" actions when the agent is in a given state\n","def possible_actions(state):\n","    current_state_row = R[state,]\n","    possible_act = ql.where(current_state_row >0)[1]\n","    return possible_act\n","\n","# Get available actions in the current state\n","PossibleAction = possible_actions(agent_s_state)\n","print(PossibleAction)\n","\n","# This function chooses at random which action to be performed within the range \n","# of all the available actions.\n","def ActionChoice(available_actions_range):\n","    if(sum(PossibleAction)>0):\n","        next_action = int(ql.random.choice(PossibleAction,1))\n","    if(sum(PossibleAction)<=0):\n","        next_action = int(ql.random.choice(5,1))\n","        print(next_action)\n","    return next_action\n","\n","# Sample next action to be performed\n","action = ActionChoice(PossibleAction)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0wvY6Yh5ikcI"},"source":["## The Bellman equation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XR1k5KQQivXA","colab":{}},"source":["# A version of the Bellman equation for reinforcement learning using the Q function\n","# This reinforcement algorithm is a memoryless process\n","# The transition function T from one state to another\n","# is not in the equation below. T is done by the random choice above\n","\n","def reward(current_state, action, gamma):\n","    Max_State = ql.where(Q[action,] == ql.max(Q[action,]))[1]\n","    \n","    if Max_State.shape[0] > 1:\n","        Max_State = int(ql.random.choice(Max_State, size = 1))\n","    else:\n","        Max_State = int(Max_State)\n","    MaxValue = Q[action, Max_State]\n","    \n","    # The Bellman MDP based Q function\n","    Q[current_state, action] = R[current_state, action] + gamma * MaxValue\n","\n","# Rewarding Q matrix\n","reward(agent_s_state,action,gamma)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zafEE-asi0k4"},"source":["## Running the training episodes randomly"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_e0SAAo_i9Jw","colab":{"base_uri":"https://localhost:8080/","height":255},"outputId":"b6037ae0-8a52-4c77-b129-d465468ab09f"},"source":["# Learning over n iterations depending on the convergence of the system\n","# A convergence function can replace the systematic repeating of the process\n","# by comparing the sum of the Q matrix to that of Q matrix n-1 in the\n","# previous episode\n","for i in range(50000):\n","    current_state = ql.random.randint(0, int(Q.shape[0]))\n","    PossibleAction = possible_actions(current_state)\n","    action = ActionChoice(PossibleAction)\n","    reward(current_state,action,gamma)\n","    \n","# Displaying Q before the norm of Q phase\n","print(\"Q  :\")\n","print(Q)\n","\n","# Norm of Q\n","print(\"Normed Q :\")\n","print(Q/ql.max(Q)*100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Q  :\n","[[  0.      0.      0.      0.    258.44    0.   ]\n"," [  0.      0.      0.    321.8     0.    207.752]\n"," [  0.      0.    500.    321.8     0.      0.   ]\n"," [  0.    258.44  401.      0.    258.44    0.   ]\n"," [207.752   0.      0.    321.8     0.      0.   ]\n"," [  0.    258.44    0.      0.      0.      0.   ]]\n","Normed Q :\n","[[  0.       0.       0.       0.      51.688    0.    ]\n"," [  0.       0.       0.      64.36     0.      41.5504]\n"," [  0.       0.     100.      64.36     0.       0.    ]\n"," [  0.      51.688   80.2      0.      51.688    0.    ]\n"," [ 41.5504   0.       0.      64.36     0.       0.    ]\n"," [  0.      51.688    0.       0.       0.       0.    ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XLFxoRFJjAvY"},"source":["# Improving the program by introducing a decision-making process"]},{"cell_type":"code","metadata":{"id":"0jP9j9GtjOps","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"16b554e4-0bcf-4460-fc03-b8aea8e7f1c6"},"source":["import random\n","import numpy as np\n","# Norm of Q\n","print(\"Normed Q :\")\n","print(Q/ql.max(Q)*100)\n","Qp=Q/ql.max(Q)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Normed Q :\n","[[  0.       0.       0.       0.      51.688    0.    ]\n"," [  0.       0.       0.      64.36     0.      41.5504]\n"," [  0.       0.     100.      64.36     0.       0.    ]\n"," [  0.      51.688   80.2      0.      51.688    0.    ]\n"," [ 41.5504   0.       0.      64.36     0.       0.    ]\n"," [  0.      51.688    0.       0.       0.       0.    ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vgqzli2hi_e0","colab_type":"code","colab":{}},"source":["import random\n","import numpy as np\n","# Norm of Q\n","print(\"Normed Q :\")\n","print(Q/ql.max(Q)*100)\n","Qp=Q/ql.max(Q)\n","\"\"\"# Improving the program by introducing a decision-making process\"\"\"\n","conceptcode=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n","\n","WIP=[0,0,0,0,0,0] # *****\n","our_query=\"\"      # *****\n","\n","print(\"Sequences\")\n","\n","maxv=1000\n","mint=450\n","maxt=500\n","#sh=ql.zeros((maxv, 2))\n","for i in range(0,maxv):\n","    for w in range(0,6):\n","      WIP[w]=random.randint(0,100)\n","    print(WIP)  \n","    print(\"\\n\")\n","    if(np.sum(WIP)>mint and np.sum(WIP)<maxt):\n","      print(mint,maxt)\n","      print(\"Alert!\", np.sum(WIP))\n","      print(\"Mention MDP or Bellman in your comment, please\")\n","      while our_query !=\"no\" or our_query !=\"bye\":\n","        our_query=input(\"Enter your comment or question:\")\n","        if our_query==\"no\" or our_query==\"bye\":\n","          break;\n","        #print(our_query)\n","        vresponse=dialog(our_query)\n","        print(vresponse)\n","      decision=input(\"Do you want to continue(enter yes) or stop(enter no) to work with your department before letting the program make a decision:\")\n","      if(decision==\"no\"):\n","        break\n","      mint=460\n","      maxt=470\n","    nextc=-1\n","    nextci=-1\n","    origin=ql.random.randint(0,6)\n","     \n","    print(\" \")\n","    print(conceptcode[int(origin)])\n","      \n","    for se in range(0,6):\n","        if(se==0):\n","            po=origin\n","        if(se>0):\n","            po=nextci\n","        for ci in range(0,6):\n","            maxc=Q[po,ci]\n","            maxp=Qp[po,ci]\n","            if(maxc>=nextc):\n","                nextc=maxc\n","                nextp=maxp\n","                nextci=ci\n","                #conceptprob[int(nextci)]=nextp *****\n","        if(nextci==po):\n","            break;\n","        print(conceptcode[int(nextci)])              \n","print(\"\\n\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pop5UZwLCFqH","colab_type":"text"},"source":["# WEB \n","\n","[ML Explanation Consult](https://console.dialogflow.com/api-client/demo/embedded/6ba8785d-6b3b-40de-8a2c-fdba7939c220)\n","\n","[ML Explanation Consult and Share](https://bot.dialogflow.com/6ba8785d-6b3b-40de-8a2c-fdba7939c220)\n","\n","      "]}]}